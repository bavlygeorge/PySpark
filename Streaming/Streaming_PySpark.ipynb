{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcXGl7FCaYIP"
   },
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqwyRqR-aZlm"
   },
   "source": [
    "### Streaming from TCP Socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RO3rBNyham7p"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# logging.debug(\"-> start\")\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Read lines over a network stream\")\\\n",
    "        .master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3K1aRW5akJe"
   },
   "source": [
    "### Print Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T0Ka-uF-akre"
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMeDLWdjarSP"
   },
   "source": [
    "### After processing, you can write the DataFrame to console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j_QcRC98asO-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = df.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXv7fZGSO7cD"
   },
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2k9D8n-JS_P"
   },
   "source": [
    "letâ€™s create a streaming DataFrame that represents text data received from a server listening on localhost:9999, and transform the DataFrame to calculate word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxtuiijIJXea"
   },
   "source": [
    "### Create DataFrame representing the stream of input lines from connection to localhost:9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FgGskzVHJai9"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Read lines over a network stream\")\\\n",
    "        .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "lines = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfP3TwpWJdkY"
   },
   "source": [
    "### Split the lines into words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8a3wjmaUJgfR"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "words = lines.select(explode(split(lines.value,' ')).alias('word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGS9HgfzJowv"
   },
   "source": [
    "### Generate running word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b2FRcEbSJnW_"
   },
   "outputs": [],
   "source": [
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asH2FPJOJ1iK"
   },
   "source": [
    "### Start running the query that prints the running counts to the console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WI2791DnJ3r0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = wordCounts.writeStream.outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gu1Jz0QuLI8S"
   },
   "source": [
    "### Read csv file \"test1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i_AxKzzVLIbf"
   },
   "outputs": [],
   "source": [
    "inputDirectory = '/home/ubuntu/pyspark/ds/'\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "recordSchema = StructType([StructField('Name', StringType(), True),\n",
    "                           StructField('Departments', StringType(), True),\n",
    "                           StructField('salary', IntegerType(), True)])\n",
    "\n",
    "df2 = spark.readStream.format(\"csv\") \\\n",
    "     .schema(recordSchema) \\\n",
    "     .load(inputDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJYbRNu7NmIQ"
   },
   "source": [
    "### Writing Spark Streaming to Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_jzZhnpFOTu-",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = df2.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "H_W_Session_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
